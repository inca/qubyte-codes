---
{
  "datetime": "2019-04-11T17:13:29Z",
  "updatedAt": null,
  "title": "Kubernetes network policies in the real world",
  "description": "I recently had to implement network policics for a couple of kubernetes clusters. There's reasonable documentation, and plenty of blogposts out there, but I still found myself figuring it out by trial and error.",
  "tags": [
    "WebServices",
    "Kubernetes"
  ]
}
---
In this post you will learn what a kubernetes network policy is for, how to
configure them to limit ingress and egress, and some of their limitations.

The first thing to know is that network policies are easy to write as you go
along, but more dangerous to add in retrospect. If you know you're going to need
them at some point, don't wait to add them!

Kubernetes network policies allow one to configure which pods, namespaces, IP
addresses, and ports incoming (ingress) connections may come from, and outgoing
(egress) connections may go to. By default containers in pods in your cluster
may make connections to anywhere at all, and for security reasons this may be
undesireable. Even within a cluster it can be good to limit communication,
formalising intra-cluster communication and complicating things for intruders.

Note however, that traffic may _only_ be limited by in the aforementioned ways.
You _can't_ limit by domain. For communication between pods and entities outside
of your cluster this can be a drawback of network policies, since it means you
don't know when the IP address of an entity may change. For such entities, the
best you may be able to do with network policies is limit the ports your pod can
use with a policy.

As an example, consider a cluster consisting of two pods. A frontend pod allows
connections from the outside world (ingress from anywhere), but only allows
connections to an application server (egress to a pod only). The application
server may only accept ingress from the frontend pod, and egress to a database.
The database lives outside the cluster, and we have an IP and a port for it.

A wide-open set of network policies, which is effectively the default looks
like:

```yaml
apiVersion: v1
kind: List
items:
- kind: NetworkPolicy
  metadata:
    name: frontend
    namespace: default
  spec:
    podSelector:
      matchLabels:
        app: frontend
    policyTypes:
      - Ingress
      - Egress
    ingress:
    - {} # FROM ANYWHERE
    egress:
    - {} # TO ANYWHERE
- kind: NetworkPolicy
  metadata:
    name: backend
    namespace: default
  spec:
    podSelector:
      matchLabels:
        app: backend
    policyTypes:
      - Ingress
      - Egress
    ingress:
    - {} # FROM ANYWHERE
    egress:
    - {} # TO ANYWHERE
```

The pods are in the default namespace and selected by an `app` label, with an
appropriate value. There's not much going on here. These are a baseline. I've
annotated the `{}` used to mean "anything", since this wasn't obvious to me.

Let's start from the frontend. Ingress is fine, but egress needs to be limited.

```yaml
kind: NetworkPolicy
metadata:
  name: frontend
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: frontend
  policyTypes:
    - Ingress
    - Egress
  ingress:
  - {}
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: backend
```

The egress has been replaced with a list with a single entry. This entry is a
pod selector which selects the backend pods by their `app` label.

For the backend server things are a little more complex. The database it
connects to is at the address `10.10.10.10:5432`. We know the IP and the port,
so we can use an IP range and port to allow it. We also restict ingress to the
frontend pods.

```yaml
kind: NetworkPolicy
metadata:
  name: backend
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
    - Ingress
    - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
  egress:
  - to:
    - ipBlock:
        cidr: 10.10.10.10/32
    ports:
      port: 5432
      protocol: TCP
```

The IP address has to be given to the network policy in the form of a
[CIDR][CIDR] address range. The `/32` limits it to a single address. I struggle
with YAML (I think it's a bad language to use for configuration, alas...), so
that egress looks like the following in JSON:

```json
[
  {
    "to": [
      { "ipBlock": { "cidr": "10.10.10.10/32" } }
    ],
    "ports": [
      { "port": 5432, "protocol": "TCP" }
    ]
  }
]
```

Which makes it more obvious the destination addresses are listed separate to the
ports. That's unfortunately true, and a quirk it took me some time to get my
head around, for lack of documented examples. When ports are not stated, any are
allowed. If you need to limit access by port, you must explicitly list every
port to allow. There is no syntax for ranges, and no way to specify individual
ports to deny.

These policies are already much better, so what did I mean by _real world_?

It's common for a kubernetes cluster to be hosted by a big platform, like
[GCP][GCP], [AWS][AWS], or [Azure][Azure]. Such platforms usually provide APIs
for things like file storage (often referred to as _buckets_) and their own
database alternatives. Since these tend to be provided as HTTP based APIs, it'd
be ideal to add an allow rule for a particular domain. Unfortunately we can't do
that on the network policy layer. Service meshes such as [istio][istio] can
provide this higher layer firewalling. For network policies we have to make do
with allowing port 443 (HTTPS).

```yaml
apiVersion: v1
kind: List
items:
- kind: NetworkPolicy
  metadata:
    name: frontend
    namespace: default
  spec:
    podSelector:
      matchLabels:
        app: frontend
    policyTypes:
      - Ingress
      - Egress
    ingress:
    - {}
    egress:
    - to:
      - podSelector:
          matchLabels:
            app: backend
    - ports:
      - port: 53
        protocol: UDP
      - port: 53
        protocol: TCP
      - port: 443
        protocol: TCP
- kind: NetworkPolicy
  metadata:
    name: backend
    namespace: default
  spec:
    podSelector:
      matchLabels:
        app: backend
    policyTypes:
      - Ingress
      - Egress
    ingress:
    - from:
      - podSelector:
          matchLabels:
            app: frontend
    egress:
    - to:
      - ipBlock:
          cidr: 10.10.10.10/32
      ports:
        port: 5432
        protocol: TCP
    - ports:
      - port: 53
        protocol: UDP
      - port: 53
        protocol: TCP
      - port: 443
        protocol: TCP
```

In both cases we've allowed egress to any destination for ports 53 (TCP and UDP)
and 443 (TCP). As mentioned 443 is for HTTPS connections. 53 is to allow egress
to DNS servers (necessary to resolve an IP given a domain, even within the
cluster).

A note on portability... The above network policies look good, except for the IP
range. If you have multiple clusters (for example, you might have a staging
cluster), then these policies are no longer portable. You _could_ use the same
database server for both environments, but this is probably a bad idea.

Since the IP block is problematic, we can either template it somehow
([kustomize][kustomize] is the option which ships with kubectl), or we can
loosen the backend policy slightly and allow egress to anywhere on 5432.

```yaml
kind: NetworkPolicy
metadata:
  name: backend
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
    - Ingress
    - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
  egress:
  - ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
    - port: 443
      protocol: TCP
    - port: 5432
      protocol: TCP
```

As with all things, it's a balance. It may be that you can tolerate egress to
anywhere on particular ports. If you can't, then you might be stuck with using
IP addresses or istio. In an ideal world it would be possible to feed these from
a `ConfigMap`. That way your pods and the network policies could draw this
information from the same place.

The saving grace is that, when bootstrapping a cluster, network policies can be
created last, when IP addresses of databases are known. If you're using a tool
such as [Terraform][Terraform], this order of events can be exploited.

[CIDR]: https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#CIDR_notation
[GCP]: https://cloud.google.com/
[AWS]: https://aws.amazon.com/
[Azure]: https://azure.microsoft.com/
[istio]:  https://istio.io/
[Terraform]: https://www.terraform.io
[kustomize]: https://kustomize.io/
